{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient, dsl, Input, Output, command, spark\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import AmlCompute, UserIdentityConfiguration\n",
    "from azure.ai.ml.constants import InputOutputModes\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load the environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# authenticate\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id = os.environ.get('AZURE_SUBSCRIPTION_ID'),\n",
    "    resource_group_name = os.environ.get('AZURE_RESOURCE_GROUP'),\n",
    "    workspace_name = os.environ.get('AZURE_WORKSPACE'),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read sample data from the repository\n",
    "df = pd.read_parquet('../data/policies.parquet')\n",
    "\n",
    "# write to blob storage\n",
    "container = os.environ.get('BLOB_CONTAINER')\n",
    "storage_account_name = os.environ.get('STORAGE_ACCOUNT_NAME')\n",
    "storage_account_key = os.environ.get('STORAGE_ACCOUNT_KEY')\n",
    "\n",
    "file_uri = f'abfs://{container}@{storage_account_name}.dfs.core.windows.net'\n",
    "\n",
    "#df.to_parquet(file_uri+'/policies.pq', \n",
    "              #engine=\"pyarrow\", \n",
    "              #storage_options = {'account_key' : storage_account_key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name assigned to the compute cluster\n",
    "cpu_compute_target = \"dev-cluster\"\n",
    "\n",
    "try:\n",
    "    # let's see if the compute target already exists\n",
    "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
    "    print(\n",
    "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "\n",
    "    # Let's create the Azure Machine Learning compute object with the intended parameters\n",
    "    cpu_cluster = AmlCompute(\n",
    "        name=cpu_compute_target,\n",
    "        # Azure Machine Learning Compute is the on-demand VM service\n",
    "        type=\"amlcompute\",\n",
    "        # VM Family\n",
    "        size=\"STANDARD_DS3_V2\",\n",
    "        # Minimum running nodes when there is no job running\n",
    "        min_instances=0,\n",
    "        # Nodes in cluster\n",
    "        max_instances=4,\n",
    "        # How many seconds will the node running after the job termination\n",
    "        idle_time_before_scale_down=180,\n",
    "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "    print(\n",
    "        f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\"\n",
    "    )\n",
    "    # Now, we pass the object to MLClient's create_or_update method\n",
    "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_data_engineering = spark(\n",
    "    name=\"data_engineering_spark\",\n",
    "    inputs={\n",
    "        \"raw_data\": Input(type=\"uri_file\", mode=\"direct\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"training_data\": Output(type=\"uri_file\", mode=\"direct\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=\"../src\",\n",
    "    entry={\"file\": \"spark_feature_eng.py\"},\n",
    "    driver_cores=2,\n",
    "    driver_memory=\"8g\",\n",
    "    executor_cores=2,\n",
    "    executor_memory=\"8g\",\n",
    "    executor_instances=2,\n",
    "    args=\"--raw_data ${{inputs.raw_data}} --training_data ${{outputs.training_data}}\", \n",
    ")\n",
    "\n",
    "model_training = command(\n",
    "    name=\"model_training\",\n",
    "    display_name=\"Model training and registration\",\n",
    "    inputs={\n",
    "        \"data\": Input(type=\"uri_file\"),\n",
    "        \"test_train_ratio\": Input(type=\"number\", default=0.2),\n",
    "        \"n_estimators\": Input(type=\"number\", default=100),\n",
    "        \"learning_rate\": Input(type=\"number\", default=0.1),\n",
    "        \"registered_model_name\": Input(type=\"string\")\n",
    "    },\n",
    "    code='../src',\n",
    "    command=\"\"\"python train.py \\\n",
    "            --data ${{inputs.data}} \n",
    "            --test_train_ratio ${{inputs.test_train_ratio}}\n",
    "            --n_estimators ${{inputs.n_estimators}}\n",
    "            --learning_rate ${{inputs.learning_rate}}\n",
    "            --registered_model_name ${{inputs.registered_model_name}}\n",
    "            \"\"\",\n",
    "    environment = \"azureml://registries/azureml/environments/sklearn-1.0/labels/latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    description=\"Model training pipeline for claims prediction\",\n",
    ")\n",
    "def training_pipeline(spark_input_data):\n",
    "    spark_step = spark_data_engineering(raw_data=spark_input_data)\n",
    "    spark_step.inputs.raw_data.mode = InputOutputModes.DIRECT\n",
    "    spark_step.outputs.training_data = Output(\n",
    "        type=\"uri_file\",\n",
    "        path=f'abfss://{container}@{storage_account_name}.dfs.core.windows.net/training_dataset',\n",
    "    )\n",
    "    spark_step.outputs.training_data.mode = InputOutputModes.DIRECT\n",
    "    spark_step.identity = UserIdentityConfiguration()\n",
    "    spark_step.resources = {\n",
    "        \"instance_type\": \"standard_e16s_v3\",\n",
    "        \"runtime_version\": \"3.3\",\n",
    "    }\n",
    "    training_step = model_training(data=spark_step.outputs.training_data,\n",
    "                                   registered_model_name=\"claims_prediction\")\n",
    "    training_step.compute=cpu_compute_target\n",
    "    \n",
    "\n",
    "\n",
    "pipeline = training_pipeline(\n",
    "    spark_input_data=Input(\n",
    "        type=\"uri_file\",\n",
    "        path=f'abfss://{container}@{storage_account_name}.dfs.core.windows.net/policies.pq',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"claims_training_pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
