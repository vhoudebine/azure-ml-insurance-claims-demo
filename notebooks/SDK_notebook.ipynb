{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient, dsl, Input, Output, command, spark\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import AmlCompute, UserIdentityConfiguration\n",
    "from azure.ai.ml.constants import InputOutputModes\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load the environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# authenticate\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id = os.environ.get('AZURE_SUBSCRIPTION_ID'),\n",
    "    resource_group_name = os.environ.get('AZURE_RESOURCE_GROUP'),\n",
    "    workspace_name = os.environ.get('AZURE_WORKSPACE'),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read sample data from the repository\n",
    "df = pd.read_parquet('../data/policies.parquet')\n",
    "\n",
    "# write to blob storage\n",
    "container = os.environ.get('BLOB_CONTAINER')\n",
    "storage_account_name = os.environ.get('STORAGE_ACCOUNT_NAME')\n",
    "storage_account_key = os.environ.get('STORAGE_ACCOUNT_KEY')\n",
    "\n",
    "file_uri = f'abfs://{container}@{storage_account_name}.dfs.core.windows.net'\n",
    "\n",
    "#df.to_parquet(file_uri+'/policies.pq', \n",
    "              #engine=\"pyarrow\", \n",
    "              #storage_options = {'account_key' : storage_account_key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name assigned to the compute cluster\n",
    "cpu_compute_target = \"dev-cluster\"\n",
    "\n",
    "try:\n",
    "    # let's see if the compute target already exists\n",
    "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
    "    print(\n",
    "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "\n",
    "    # Let's create the Azure Machine Learning compute object with the intended parameters\n",
    "    cpu_cluster = AmlCompute(\n",
    "        name=cpu_compute_target,\n",
    "        # Azure Machine Learning Compute is the on-demand VM service\n",
    "        type=\"amlcompute\",\n",
    "        # VM Family\n",
    "        size=\"STANDARD_DS3_V2\",\n",
    "        # Minimum running nodes when there is no job running\n",
    "        min_instances=0,\n",
    "        # Nodes in cluster\n",
    "        max_instances=4,\n",
    "        # How many seconds will the node running after the job termination\n",
    "        idle_time_before_scale_down=180,\n",
    "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "    print(\n",
    "        f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\"\n",
    "    )\n",
    "    # Now, we pass the object to MLClient's create_or_update method\n",
    "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_data_engineering = spark(\n",
    "    name=\"data_engineering_spark\",\n",
    "    inputs={\n",
    "        \"raw_data\": Input(type=\"uri_file\", mode=\"direct\"),\n",
    "    },\n",
    "    outputs={\n",
    "        \"training_data\": Output(type=\"uri_file\", mode=\"direct\"),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=\"../src\",\n",
    "    entry={\"file\": \"spark_feature_eng.py\"},\n",
    "    driver_cores=2,\n",
    "    driver_memory=\"8g\",\n",
    "    executor_cores=2,\n",
    "    executor_memory=\"8g\",\n",
    "    executor_instances=2,\n",
    "    args=\"--raw_data ${{inputs.raw_data}} --training_data ${{outputs.training_data}}\", \n",
    ")\n",
    "\n",
    "model_training = command(\n",
    "    name=\"model_training\",\n",
    "    display_name=\"Model training and registration\",\n",
    "    inputs={\n",
    "        \"data\": Input(type=\"uri_file\"),\n",
    "        \"test_train_ratio\": Input(type=\"number\", default=0.2),\n",
    "        \"n_estimators\": Input(type=\"number\", default=100),\n",
    "        \"learning_rate\": Input(type=\"number\", default=0.1),\n",
    "        \"registered_model_name\": Input(type=\"string\")\n",
    "    },\n",
    "    code='../src',\n",
    "    command=\"\"\"python train.py \\\n",
    "            --data ${{inputs.data}} \n",
    "            --test_train_ratio ${{inputs.test_train_ratio}}\n",
    "            --n_estimators ${{inputs.n_estimators}}\n",
    "            --learning_rate ${{inputs.learning_rate}}\n",
    "            --registered_model_name ${{inputs.registered_model_name}}\n",
    "            \"\"\",\n",
    "    environment = \"azureml://registries/azureml/environments/sklearn-1.0/labels/latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    description=\"Model training pipeline for claims prediction\",\n",
    ")\n",
    "def training_pipeline(spark_input_data):\n",
    "    spark_step = spark_data_engineering(raw_data=spark_input_data)\n",
    "    spark_step.inputs.raw_data.mode = InputOutputModes.DIRECT\n",
    "    spark_step.outputs.training_data = Output(\n",
    "        type=\"uri_file\",\n",
    "        path=f'abfss://{container}@{storage_account_name}.dfs.core.windows.net/training_dataset',\n",
    "    )\n",
    "    spark_step.outputs.training_data.mode = InputOutputModes.DIRECT\n",
    "    spark_step.identity = UserIdentityConfiguration()\n",
    "    spark_step.resources = {\n",
    "        \"instance_type\": \"standard_e16s_v3\",\n",
    "        \"runtime_version\": \"3.3\",\n",
    "    }\n",
    "    training_step = model_training(data=spark_step.outputs.training_data,\n",
    "                                   registered_model_name=\"claims_prediction\")\n",
    "    training_step.compute=cpu_compute_target\n",
    "    \n",
    "\n",
    "\n",
    "pipeline = training_pipeline(\n",
    "    spark_input_data=Input(\n",
    "        type=\"uri_file\",\n",
    "        path=f'abfss://{container}@{storage_account_name}.dfs.core.windows.net/policies.pq',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelineJob' object has no attribute 'stream'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# submit the pipeline job\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pipeline_job \u001b[38;5;241m=\u001b[39m ml_client\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mcreate_or_update(\n\u001b[1;32m      3\u001b[0m     pipeline,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Project's name\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     experiment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaims_training_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mpipeline_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelineJob' object has no attribute 'stream'"
     ]
    }
   ],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"claims_training_pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: shy_steelpan_fhy0xstx8d\n",
      "Web View: https://ml.azure.com/runs/shy_steelpan_fhy0xstx8d?wsid=/subscriptions/6c065ea7-65cd-4a34-8e2a-3e21ad4a8e9f/resourcegroups/vince-rg/workspaces/vince-dev\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2024-04-05 21:18:40Z] Submitting 1 runs, first five are: 908e88ef:e5c1cb08-d16a-4c67-9b4e-9c2b9f9a7ad7\n",
      "[2024-04-05 21:24:39Z] Completing processing run id e5c1cb08-d16a-4c67-9b4e-9c2b9f9a7ad7.\n",
      "[2024-04-05 21:24:40Z] Submitting 1 runs, first five are: 98ec8e6d:ce65be52-bce1-4b0e-ad38-406fba02a781\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/python310/lib/python3.10/site-packages/azure/ai/ml/operations/_job_ops_helper.py:270\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[0;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[1;32m    269\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m--> 270\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_wait_before_polling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpoll_start_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m _current_details: RunDetails \u001b[38;5;241m=\u001b[39m run_operations\u001b[38;5;241m.\u001b[39mget_run_details(job_name)  \u001b[38;5;66;03m# TODO use FileWatcher\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mJobException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Wait until the job completes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python310/lib/python3.10/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/miniconda3/envs/python310/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:275\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[0;32m--> 275\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python310/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:788\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[38;5;241m=\u001b[39mjob_object\u001b[38;5;241m.\u001b[39mid)\n\u001b[0;32m--> 788\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_logs_until_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runs_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requests_pipeline\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python310/lib/python3.10/site-packages/azure/ai/ml/operations/_job_ops_helper.py:348\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[0;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    342\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe output streaming for the run interrupted.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut the run is still executing on the compute target. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetails for canceling the run can be found here: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://aka.ms/aml-docs-cancel-run\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    347\u001b[0m     )\n\u001b[0;32m--> 348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JobException(\n\u001b[1;32m    349\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_message,\n\u001b[1;32m    350\u001b[0m         target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mJOB,\n\u001b[1;32m    351\u001b[0m         no_personal_data_message\u001b[38;5;241m=\u001b[39merror_message,\n\u001b[1;32m    352\u001b[0m         error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mUSER_ERROR,\n\u001b[1;32m    353\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mJobException\u001b[0m: The output streaming for the run interrupted.\nBut the run is still executing on the compute target. \nDetails for canceling the run can be found here: https://aka.ms/aml-docs-cancel-run"
     ]
    }
   ],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
